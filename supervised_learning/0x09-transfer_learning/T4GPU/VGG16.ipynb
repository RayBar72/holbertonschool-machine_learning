{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOhQqm5/iQlUFIS22Vg/s3D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Modulus that trains a CIFAR 10 dataset\n","## Uses VGG16\n","### Very Deep Convolutional Networks for Large-Scale Image Recognition\n","* Karen Simonyan, Andrew Zisserman*\n","In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."],"metadata":{"id":"PvVruZI7PZwO"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0mgrq4u6PXbH","executionInfo":{"status":"ok","timestamp":1665061427373,"user_tz":180,"elapsed":336443,"user":{"displayName":"Raymundo Barrera Flores","userId":"09320084639407893070"}},"outputId":"ac01be9f-c4cc-484e-edc5-778ef4e2ebc2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 2s 0us/step\n","170508288/170498071 [==============================] - 2s 0us/step\n","(50000, 32, 32, 3) (50000, 1)\n","(50000, 32, 32, 3) (50000, 10)\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 0s 0us/step\n","58900480/58889256 [==============================] - 0s 0us/step\n","Epoch 1/5\n","1563/1563 [==============================] - ETA: 0s - loss: 1.0132 - acc: 0.6754\n","Epoch 1: val_acc improved from -inf to 0.74780, saving model to cifar10.h5\n","1563/1563 [==============================] - 69s 36ms/step - loss: 1.0132 - acc: 0.6754 - val_loss: 0.7961 - val_acc: 0.7478\n","Epoch 2/5\n","1563/1563 [==============================] - ETA: 0s - loss: 0.6833 - acc: 0.7867\n","Epoch 2: val_acc did not improve from 0.74780\n","1563/1563 [==============================] - 53s 34ms/step - loss: 0.6833 - acc: 0.7867 - val_loss: 0.9347 - val_acc: 0.7025\n","Epoch 3/5\n","1563/1563 [==============================] - ETA: 0s - loss: 0.6389 - acc: 0.8069\n","Epoch 3: val_acc did not improve from 0.74780\n","1563/1563 [==============================] - 53s 34ms/step - loss: 0.6389 - acc: 0.8069 - val_loss: 1.1362 - val_acc: 0.6100\n","Epoch 4/5\n","1563/1563 [==============================] - ETA: 0s - loss: 0.6574 - acc: 0.8062\n","Epoch 4: val_acc improved from 0.74780 to 0.79450, saving model to cifar10.h5\n","1563/1563 [==============================] - 54s 34ms/step - loss: 0.6574 - acc: 0.8062 - val_loss: 0.8529 - val_acc: 0.7945\n","Epoch 5/5\n","1563/1563 [==============================] - ETA: 0s - loss: 0.8865 - acc: 0.8073\n","Epoch 5: val_acc did not improve from 0.79450\n","1563/1563 [==============================] - 53s 34ms/step - loss: 0.8865 - acc: 0.8073 - val_loss: 0.9846 - val_acc: 0.7559\n"]}],"source":["import tensorflow.keras as K\n","import tensorflow as tf\n","\n","\n","def preprocess_data(X, Y):\n","    X_p = K.applications.vgg16.preprocess_input(X)\n","    Y_p = K.utils.to_categorical(Y, 10)\n","    return X_p, Y_p\n","\n","(xtn, ytn), (xtt, ytt) = K.datasets.cifar10.load_data()\n","print(xtn.shape, ytn.shape)\n","xtn, ytn = preprocess_data(xtn, ytn)\n","xtt, ytt = preprocess_data(xtt, ytt)\n","print(xtn.shape, ytn.shape)\n","\n","inputs_t = K.Input(shape=(32, 32, 3))\n","model = K.applications.vgg16.VGG16(include_top=False,\n","                                     input_tensor=inputs_t)\n","model_p = K.Sequential()\n","# model_p.add(K.layers.UpSampling2D((7, 7)))\n","model_p.add(model)\n","# model_p.add(K.layers.MaxPool2D(pool_size=(7, 7)))\n","model_p.add(K.layers.Flatten())\n","model_p.add(K.layers.Dense(10, activation='softmax'))\n","\n","check = K.callbacks.ModelCheckpoint(filepath='cifar10.h5',\n","                                    monitor='val_acc',\n","                                    mode='max',\n","                                    verbose=1,\n","                                    save_best_only=True)\n","\n","model_p.compile(optimizer=K.optimizers.RMSprop(learning_rate=1e-4),\n","                loss='categorical_crossentropy',\n","                metrics=['acc'])\n","\n","model_p.fit(xtn,\n","            ytn,\n","            validation_data=(xtt, ytt),\n","            batch_size=32,\n","            epochs=5,\n","            verbose=1,\n","            callbacks=[check])\n","\n","model_p.save('/content/drive/MyDrive/Colab Notebooks/supervised_learning/tests for 0x09/cifar10_VGG16.h5')\n"]}]}